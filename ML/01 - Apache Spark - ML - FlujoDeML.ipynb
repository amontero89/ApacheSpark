{"cells":[{"cell_type":"markdown","source":["# Apache Spark - ML - FlujoDeML"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b427e556-0872-4dcc-9f0b-6739bd5011c7"}}},{"cell_type":"markdown","source":["<p><strong>Objetivo: </strong> El objetivo de este cuaderno es crear un flujo sencillo de machine learning</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3d35310-cd72-43f7-92f0-0aa308209e2a"}}},{"cell_type":"markdown","source":["## Cargar de datos en un Dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa4f5b5-6254-4238-9c8d-9e911a6eda34"}}},{"cell_type":"markdown","source":["Este conjunto de datos consta de una etiqueta categórica con dos valores (buenos o malos), una variable categórica (color) y dos variables numéricas.\n\nSi bien los datos son sintéticos, imaginemos que este conjunto de datos representa la salud del cliente de una empresa. La columna \"color\" representa una calificación de salud categórica hecha por un representante de servicio al cliente. La columna \"laboratorio\" representa la verdadera salud del cliente. Los otros dos valores son algunas medidas numéricas de actividad dentro de una aplicación (por ejemplo, minutos de permanencia en el sitio y compras).\n\nSupongamos que queremos entrenar un modelo de clasificación en el que esperamos predecir una variable binaria, la etiqueta, a partir de los otros valores."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b492c75-e3c1-427c-b0dc-3b4b918727d2"}}},{"cell_type":"code","source":["%fs ls /databricks-datasets/definitive-guide/data/simple-ml"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a57bfc33-6169-4be8-a06a-2e470b7f3183"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/definitive-guide/data/simple-ml/_SUCCESS","_SUCCESS",0],["dbfs:/databricks-datasets/definitive-guide/data/simple-ml/part-r-00000-f5c243b9-a015-4a3b-a4a8-eca00f80f04c.json","part-r-00000-f5c243b9-a015-4a3b-a4a8-eca00f80f04c.json",7560]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/definitive-guide/data/simple-ml/_SUCCESS</td><td>_SUCCESS</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/data/simple-ml/part-r-00000-f5c243b9-a015-4a3b-a4a8-eca00f80f04c.json</td><td>part-r-00000-f5c243b9-a015-4a3b-a4a8-eca00f80f04c.json</td><td>7560</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.json(\"/databricks-datasets/definitive-guide/data/simple-ml\")\ndf.orderBy(\"value2\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8a4e957-88ef-457f-8946-f2e56e04cfdd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+----+------+------------------+\n|color| lab|value1|            value2|\n+-----+----+------+------------------+\n|green|good|     1|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n|green|good|    12|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n|  red| bad|     2|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|green|good|     1|14.386294994851129|\n|green|good|    12|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n|green|good|    12|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+----+------+------------------+\n|color| lab|value1|            value2|\n+-----+----+------+------------------+\n|green|good|     1|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|green| bad|    16|14.386294994851129|\n|green|good|    12|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n|  red| bad|     2|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|green|good|     1|14.386294994851129|\n|green|good|    12|14.386294994851129|\n| blue| bad|     8|14.386294994851129|\n|  red|good|    35|14.386294994851129|\n| blue| bad|    12|14.386294994851129|\n|  red| bad|    16|14.386294994851129|\n|green|good|    12|14.386294994851129|\n+-----+----+------+------------------+\nonly showing top 20 rows\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Transformaciones"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ead1a57c-db9f-43c8-9289-34599a0f709f"}}},{"cell_type":"markdown","source":["El conjunto de datos actual no cumple con el requisito de estar en formato de Vector y, por lo tanto, debemos transformarlo al formato adecuado.\n\nPara lograr esto en nuestro ejemplo, vamos a especificar una RFormula. Este es un lenguaje declarativo para especificar transformaciones de aprendizaje automático y es fácil de usar una vez que comprende la sintaxis.\n\nLos operadores básicos de RFormula son:\n<p>\n<p>\"~\" Destino y términos separados</p>\n<p>\"+\" Términos de Concat; \"+ 0\" significa eliminar la intersección (esto significa que la intersección y de la línea que ajustaremos será 0)</p>\n<p>\"-\" Eliminar un término; \"- 1\" significa eliminar la intersección (esto significa que la intersección y de la línea que vamos a ajustar será 0; sí, esto hace lo mismo que \"+ 0\"</p>\n<p>\":\" Interacción (multiplicación de valores numéricos o valores categóricos binarizados)</p>\n<p>\".\" Todas las columnas excepto la variable objetivo / dependiente</p>\n</p>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfe71ef1-9c01-426e-8474-01619db854d4"}}},{"cell_type":"markdown","source":["Para especificar transformaciones con esta sintaxis, necesitamos importar la clase RFormula. Luego pasamos por el proceso de definir nuestra fórmula. En este caso, queremos usar todas las variables disponibles (el \".\") Y también agregar las interacciones entre valor1 y color y valor2 y color, tratándolas como características nuevas:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf996bfa-9c00-42e6-b38c-3198c937235c"}}},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\nsupervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6a603a3-7960-464b-bfd0-efb86c6d5d31"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["El siguiente paso es ajustar el transformador RFormula a los datos para que descubra los posibles valores de cada columna.\n\nNo todos los transformadores tienen este requisito, pero debido a que RFormula manejará automáticamente las variables categóricas por nosotros, necesita determinar qué columnas son categóricas y cuáles no, así como cuáles son los valores distintos de las columnas categóricas.\n\nPor esta razón, tenemos que llamar al método fit. Una vez que llamamos a fit, devuelve una versión \"entrenada\" de nuestro transformador que luego podemos usar para transformar nuestros datos.\n\nLuego llamamos a transform en ese objeto para transformar nuestros datos de entrada en los datos de salida esperados."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29547c28-244f-4b87-94a3-39d56c5a8d14"}}},{"cell_type":"code","source":["fittedRF = supervised.fit(df) # Ajusta\npreparedDF = fittedRF.transform(df) # Transforma\npreparedDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11077b04-a475-4e39-bcf6-38836b6fc473"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+----+------+------------------+--------------------+-----+\n|color| lab|value1|            value2|            features|label|\n+-----+----+------+------------------+--------------------+-----+\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n+-----+----+------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+----+------+------------------+--------------------+-----+\n|color| lab|value1|            value2|            features|label|\n+-----+----+------+------------------+--------------------+-----+\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n+-----+----+------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["En la salida podemos ver el resultado de nuestra transformación: una columna llamada características que tiene nuestros datos sin procesar.\n\nLo que sucede detrás de escena es bastante simple: RFormula inspecciona nuestros datos durante la llamada de ajuste y genera un objeto que transformará nuestros datos de acuerdo con la fórmula especificada.\n\nCuando usamos este transformador, Spark convierte automáticamente nuestra variable categórica en Dobles para que podamos ingresarla en un modelo de aprendizaje automático.\n\nEn particular, asigna un valor numérico a cada color posible.\ncategoría, crea características adicionales para las variables de interacción entre colores y valor1 / valor2, y las coloca todas en un solo vector."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bc2ae8f-7b6f-4c6e-90d9-753437ebcc18"}}},{"cell_type":"markdown","source":["Creemos ahora un conjunto de prueba simple basado en una división aleatoria de los datos:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c11c50e-fa58-4af3-b957-c4fbd2030721"}}},{"cell_type":"code","source":["train, test = preparedDF.randomSplit([0.7, 0.3])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ecb0e02-bcd5-4957-ba7a-fa689a88d32e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Estimators"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a09fe42-8434-4a1a-9635-8a009c30f51c"}}},{"cell_type":"markdown","source":["En este caso usaremos un algoritmo de clasificación llamado regresión logística.\n\nPara crear nuestro clasificador, creamos una instancia de LogisticRegression, usando la configuración predeterminada o los hiperparámetros.\n\nLuego configuramos las columnas de etiquetas y las columnas de características."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d287b901-ac74-48c7-80ea-d62ee69c1a70"}}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f829b0a-39d3-449e-97e8-bddc44904cc4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Antes de comenzar a entrenar este modelo, inspeccionemos los parámetros.\n\nEste método muestra una explicación de todos los parámetros para la implementación de Spark de la regresión logística.\n\nEl método \"explainParams\" existe en todos los algoritmos disponibles en MLlib."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7793a8bf-ecf2-4070-b522-1e83c2f12b07"}}},{"cell_type":"code","source":["print(lr.explainParams())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"900fa03a-7bdc-4175-8091-257a9bcdf90d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\nmaxIter: max number of iterations (>= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\nmaxIter: max number of iterations (>= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Al crear una instancia de un algoritmo no entrenado, llega el momento de ajustarlo a los datos (entrenarlo). En este caso, esto devuelve un LogisticRegressionModel.\n\nEste código iniciará un trabajo de Spark para entrenar el modelo. A diferencia de las transformaciones, el ajuste de un modelo de aprendizaje automático es ansioso y se realiza de inmediato."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"248c67a5-89ed-43eb-98ec-6081c7d738c6"}}},{"cell_type":"code","source":["fittedLR = lr.fit(train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab7501ad-ed35-4d1a-ade0-9b9a5eb47100"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Una vez completado, puede usar el modelo para hacer predicciones. Lógicamente, esto significa transformar características en etiquetas.\n\nHacemos predicciones con el método transform. Por ejemplo, podemos transformar nuestro conjunto de datos de entrenamiento para ver qué etiquetas asignó nuestro modelo a los datos de entrenamiento y cómo se comparan con los resultados reales.\n\nRealicemos esa predicción con el siguiente fragmento de código:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"534ade73-8c65-45e0-9ba1-7b33e2fa7009"}}},{"cell_type":"code","source":["fittedLR.transform(train).select(\"label\", \"prediction\").show(50)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee5de9e5-cfdd-4e9f-9470-63aa116dc1a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+----------+\n|label|prediction|\n+-----+----------+\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n+-----+----------+\nonly showing top 50 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+----------+\n|label|prediction|\n+-----+----------+\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n+-----+----------+\nonly showing top 50 rows\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["fittedLR.transform(test).select(\"label\", \"prediction\").show(50)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ca4a478-3fb0-493a-a842-364cc962e00b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+----------+\n|label|prediction|\n+-----+----------+\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n+-----+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+----------+\n|label|prediction|\n+-----+----------+\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n+-----+----------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Nuestro siguiente paso sería evaluar manualmente este modelo y calcular métricas de rendimiento como la tasa de verdaderos positivos, la tasa de falsos negativos, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"197bad2a-9e39-42b8-92ab-cd0e0dc4f4f7"}}},{"cell_type":"markdown","source":["## Pipelines"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef5cc685-f7c7-41af-af98-38d74227b3aa"}}},{"cell_type":"markdown","source":["Como probablemente haya notado, si está realizando muchas transformaciones, escribir todos los pasos y realizar un seguimiento de DataFrames termina siendo bastante tedioso.\n\nPor eso Spark incluye el concepto Pipeline.\n\nTenga en cuenta que es esencial que las instancias de transformadores o modelos no se reutilicen en diferentes Pipeline. Cree siempre una nueva instancia de un modelo antes de crear otra Pipeline.\n\nPara asegurarnos de no sobreajustarnos, crearemos un conjunto de pruebas de holdout(un método de validación) y ajustaremos nuestros hiperparámetros en función de un conjunto de validación (tenga en cuenta que creamos este conjunto de validación basado en el conjunto de datos original, no en el preparedDF):"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08a020c2-f342-4837-ad49-63af3c567c61"}}},{"cell_type":"code","source":["train, test = df.randomSplit([0.7, 0.3])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"149a5c9b-b83e-4cf8-ae34-63bab1c68fb1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ahora que tiene un conjunto de entrenamiento y prueba, creemos las stages base en nuestra Pipeline.\n\nUna stage simplemente representa un transformador o un estimador. En nuestro caso, tendremos dos estimadores. La RFomula y el LogisticRegresión:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec7dcb71-dd5d-4540-8066-87e83acd4c58"}}},{"cell_type":"code","source":["rForm = RFormula()\nlr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed582fda-ecc0-4be8-8301-969ffe5bef2f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ahora, en lugar de usar manualmente nuestras transformaciones y luego ajustar nuestro modelo, simplemente las hacemos stages en la Pipeline general, como en el siguiente fragmento de código:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02559619-d928-4166-a14a-d09cecd0d419"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nstages = [rForm, lr]\npipeline = Pipeline().setStages(stages)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"748b2700-bcc7-4ba4-87e8-cad9d0cc542b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Entrenamiento y Evaluación"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"212be521-a72b-4dc9-b4f5-2e634eb02a80"}}},{"cell_type":"markdown","source":["Ahora que organizó la Pipeline, el siguiente paso es el Entrenamiento.\n\nEn este caso, no entrenaremos solo un modelo. Entrenaremos varias variaciones del modelo especificando diferentes combinaciones de hiperparámetros que nos gustaría que Spark probara.\n\nLuego, seleccionaremos el mejor modelo usando un evaluador que compara sus predicciones con nuestros datos de validación.\n\nPodemos probar diferentes hiperparámetros en toda la Pipeline, incluso en la fórmula de RF que usamos para manipular los datos sin procesar.\n\nEn nuestro ParamGridBuilder, hay tres hiperparámetros que varían de los valores predeterminados:\n<li>Dos versiones diferentes de RFormula</li>\n<li>Tres opciones diferentes para el parámetro ElasticNet</li>\n<li>Dos opciones diferentes para el parámetro de regularización</li>\nEsto nos da un total de 12 combinaciones diferentes de estos parámetros, lo que significa que entrenaremos 12 versiones diferentes de regresión logística."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"281349c1-adcd-4ab0-b73d-bc3af76598a6"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\nparams = ParamGridBuilder()\\\n.addGrid(rForm.formula, [\n\"lab ~ . + color:value1\",\n\"lab ~ . + color:value1 + color:value2\"])\\\n.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n.addGrid(lr.regParam, [0.1, 2.0])\\\n.build()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e78f15e-9c00-4bdc-82c7-335affe06059"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ahora que la cuadrícula está construida, es hora de especificar nuestro proceso de evaluación. El evaluador nos permite comparar de forma automática y objetiva varios modelos con la misma métrica de evaluación.\n\nEn este caso usaremos el BinaryClassificationEvaluator, que tiene una serie de métricas de evaluación potenciales.\n\nEn este caso usaremos areaUnderROC, que es el área total bajo la característica operativa del receptor, una medida común de desempeño de clasificación."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcfec495-6772-478e-b31f-bece00f29fe9"}}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\\\n.setMetricName(\"areaUnderROC\")\\\n.setRawPredictionCol(\"prediction\")\\\n.setLabelCol(\"label\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd5a4d04-2d70-4673-9900-2408ca6384ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ahora que tenemos una canalización que especifica cómo se deben transformar nuestros datos, realizaremos la selección del modelo para probar diferentes hiperparámetros en nuestro modelo de regresión logística y medir el éxito comparando su desempeño usando la métrica areaUnderROC."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"273cd2d9-cf7d-4b15-8d40-1eb213564d94"}}},{"cell_type":"code","source":["from pyspark.ml.tuning import TrainValidationSplit\ntvs = TrainValidationSplit()\\\n.setTrainRatio(0.75)\\\n.setEstimatorParamMaps(params)\\\n.setEstimator(pipeline)\\\n.setEvaluator(evaluator)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e35b9a12-0c9e-40aa-84b1-2ce640bf5e8f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Ejecutemos toda la Pipeline que construimos. Para revisar, la ejecución de esta canalización probará todas las versiones del modelo con el conjunto de entrenamiento:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcd3e4d1-19ef-455a-8f6d-eb24ebeba5f4"}}},{"cell_type":"code","source":["tvsFitted = tvs.fit(train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84857ba8-1742-47a8-95ec-82909d1969f8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Tambien se evalua cómo funciona el algoritmo con el conjunto de prueba:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f78f9969-e788-40a3-9185-745075fa7cd6"}}},{"cell_type":"code","source":["evaluator.evaluate(tvsFitted.transform(test))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab357264-cbf0-4287-801f-3cb3c5ada7df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[17]: 0.975","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: 0.975"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"01 - Apache Spark - ML - FlujoDeML","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1364852366998788}},"nbformat":4,"nbformat_minor":0}
